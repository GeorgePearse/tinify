# Tinify Training Configuration
# Model: Attention-based with GMM (Cheng et al., CVPR 2020)
#
# Usage:
#   tinify train image --config configs/cheng2020-anchor.yaml

domain: image

model:
  name: cheng2020-anchor
  quality: 3
  pretrained: false

dataset:
  path: /path/to/dataset
  split_train: train
  split_test: test
  patch_size: [256, 256]
  num_workers: 4

training:
  epochs: 400
  batch_size: 8  # Smaller batch size due to larger model
  test_batch_size: 32
  lmbda: 0.0067
  metric: mse
  clip_max_norm: 1.0
  cuda: true
  save: true
  save_dir: ./checkpoints/cheng2020-anchor-q3

optimizer:
  net:
    type: Adam
    lr: 0.0001
  aux:
    type: Adam
    lr: 0.001

scheduler:
  type: ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 30  # More patience for larger model
