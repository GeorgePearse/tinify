# Tinify Training Configuration
# Model: Scale Hyperprior (Ball√© et al., ICLR 2018)
#
# Usage:
#   tinify train image --config configs/bmshj2018-hyperprior.yaml

domain: image

model:
  name: bmshj2018-hyperprior
  quality: 3
  pretrained: false

dataset:
  path: /path/to/dataset
  split_train: train
  split_test: test
  patch_size: [256, 256]
  num_workers: 4

training:
  epochs: 200
  batch_size: 16
  test_batch_size: 64
  lmbda: 0.0067
  metric: mse
  clip_max_norm: 1.0
  cuda: true
  save: true
  save_dir: ./checkpoints/bmshj2018-hyperprior-q3

optimizer:
  net:
    type: Adam
    lr: 0.0001
  aux:
    type: Adam
    lr: 0.001

scheduler:
  type: ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 20
